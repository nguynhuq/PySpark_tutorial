{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing User Profiles with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use case will bring together the core concepts of Spark and use a large dataset to build a simple real-time dashboard that provides insight into customer behaviors.\n",
    "\n",
    "Contributed by: Nick Amato, Director, Technical Marketing for MapR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivering Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music streaming is a rather pervasive technology which generates massive quantities of data. This type of service is much like people would use every day on a desktop or mobile device, whether as a subscriber or a free listener (perhaps even similar to a Pandora). This will be the foundation of the use case to be explored. Data from such a streaming service will be analyzed.\n",
    "\n",
    "The basic layout consists of customers whom are logging into this service and listening to music tracks, and they have a variety of parameters:\n",
    "\n",
    "    Demographic information (gender, location, etc.)\n",
    "    Free / paid subscriber\n",
    "    Listening history; tracks selected, when, and geolocation when they were selected\n",
    "\n",
    "Python, PySpark and MLlib will be used to compute some basic statistics for a dashboard, enabling a high-level view of customer behaviors as well as a constantly updated view of the latest information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be loaded directly from a CSV file. There are a couple of steps to perform before it can be analyzed. The data will need to be transformed and loaded into a PairRDD. This is because the data consists of arrays of (key, value) tuples.\n",
    "\n",
    "The customer events-individual tracks dataset (tracks.csv) consists of a collection of events, one per line, where each event is a client listening to a track. This size is approximately 1M lines and contains simulated listener events over several months. Because this represents things that are happening at a very low level, this data has the potential to grow very large.\n",
    "\n",
    "The event, customer and track IDs show that a customer listened to a specific track. The other fields show associated information, like whether the customer was listening on a mobile device, and a geolocation. This will serve as the input into the first Spark job.\n",
    "\n",
    "The customer information dataset (cust.csv) consists of all statically known details about a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields are defined as follows:\n",
    "\n",
    "    Customer ID: a unique identifier for that customer\n",
    "    Name, gender, address, zip: the customer's associated information\n",
    "    Sign date: the date of addition to the service\n",
    "    Status: indicates whether or not the account is active (0 = closed, 1 = active)\n",
    "    Level: indicates what level of service -0, 1, 2 for Free, Silver and Gold, respectively\n",
    "    Campaign: indicates the campaign under which the user joined, defined as the following (fictional) campaigns driven by our (also fictional) marketing team:\n",
    "        NONE no campaign\n",
    "        30DAYFREE a '30 days free' trial offer\n",
    "        SUPERBOWL a Super Bowl-related program\n",
    "        RETAILSTORE an offer originating in brick-and-mortar retail stores\n",
    "        WEBOFFER an offer for web-originated customers\n",
    "\n",
    "Other datasets that would be available, but will not be used for this use case, would include:\n",
    "\n",
    "    Advertisement click history\n",
    "    Track details like title, album and artist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the right information is in place and a lot of micro-level detail is available that describes what customers listen to and when. The quickest way to get this data to a dashboard is by leveraging Spark to create summary information for each customer as well as basic statistics about the entire user base. After the results are generated, they can be persisted to a file which can be easily used for visualization with BI tools such as Tableau, or other dashboarding frameworks like C3.js or D3.js.\n",
    "\n",
    "Step one in getting started is to initialize a Spark context. Additional parameters could be passed to the SparkConf method to further configure the job, such as setting the master and the directory where the job executes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import csv\n",
    "\n",
    "#Add parameter to further configure\n",
    "conf = SparkConf().setAppName('ListenerSummarizer')\n",
    "#Initialize Spark context\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to read the CSV records with the individual track events, and make a PairRDD out of all of the rows. To convert each line of data into an array, the map() function will be used, and then reduceByKey() is called to consolidate all of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'4446',\n",
       " [[511, u'\"2014-12-04 03:35:27\"', 1, u'\"23081\"'],\n",
       "  [527, u'\"2014-11-26 21:31:06\"', 0, u'\"96142\"'],\n",
       "  [1318, u'\"2014-11-12 21:30:24\"', 1, u'\"49751\"'],\n",
       "  [1129, u'\"2014-10-21 10:17:16\"', 1, u'\"78567\"'],\n",
       "  [667, u'\"2014-12-10 09:26:40\"', 1, u'\"44622\"'],\n",
       "  [90, u'\"2014-11-24 15:33:56\"', 1, u'\"28391\"'],\n",
       "  [1291, u'\"2014-11-07 04:04:03\"', 0, u'\"55168\"'],\n",
       "  [1128, u'\"2014-12-19 02:51:00\"', 0, u'\"07501\"'],\n",
       "  [22, u'\"2014-10-09 07:55:57\"', 0, u'\"58121\"'],\n",
       "  [271, u'\"2014-11-30 14:36:02\"', 0, u'\"36663\"'],\n",
       "  [587, u'\"2014-12-10 22:23:31\"', 0, u'\"06232\"'],\n",
       "  [606, u'\"2014-12-17 22:37:04\"', 1, u'\"38037\"'],\n",
       "  [141, u'\"2014-10-05 02:55:38\"', 1, u'\"19126\"'],\n",
       "  [11, u'\"2014-12-24 22:28:50\"', 1, u'\"20650\"'],\n",
       "  [1036, u'\"2014-10-30 07:50:21\"', 0, u'\"28220\"'],\n",
       "  [1616, u'\"2014-11-28 08:56:29\"', 1, u'\"02638\"'],\n",
       "  [1375, u'\"2014-12-28 13:04:35\"', 0, u'\"67878\"'],\n",
       "  [1267, u'\"2014-11-10 13:27:23\"', 1, u'\"10028\"'],\n",
       "  [390, u'\"2014-11-22 14:38:49\"', 1, u'\"10307\"'],\n",
       "  [283, u'\"2014-10-09 12:59:21\"', 1, u'\"24121\"'],\n",
       "  [249, u'\"2014-12-29 15:05:00\"', 1, u'\"57075\"'],\n",
       "  [180, u'\"2014-11-09 04:10:09\"', 0, u'\"61030\"'],\n",
       "  [32, u'\"2014-12-21 00:37:59\"', 0, u'\"89705\"'],\n",
       "  [1196, u'\"2014-12-03 02:51:42\"', 0, u'\"25149\"'],\n",
       "  [126, u'\"2014-11-12 17:39:47\"', 1, u'\"32072\"'],\n",
       "  [616, u'\"2014-11-20 18:17:33\"', 0, u'\"76652\"'],\n",
       "  [226, u'\"2014-12-03 09:15:42\"', 1, u'\"84712\"'],\n",
       "  [1462, u'\"2014-11-18 21:15:51\"', 1, u'\"93383\"'],\n",
       "  [531, u'\"2014-12-04 00:30:36\"', 0, u'\"26047\"'],\n",
       "  [496, u'\"2014-12-22 13:25:33\"', 1, u'\"10109\"'],\n",
       "  [343, u'\"2014-11-05 07:00:02\"', 0, u'\"88213\"'],\n",
       "  [80, u'\"2014-10-30 11:18:02\"', 0, u'\"78552\"'],\n",
       "  [1503, u'\"2014-12-29 04:52:47\"', 1, u'\"62295\"'],\n",
       "  [413, u'\"2014-12-17 18:27:50\"', 1, u'\"37932\"'],\n",
       "  [316, u'\"2014-11-21 03:57:20\"', 1, u'\"17056\"'],\n",
       "  [6, u'\"2014-11-17 23:40:32\"', 0, u'\"45254\"'],\n",
       "  [503, u'\"2014-12-04 06:06:52\"', 0, u'\"28681\"'],\n",
       "  [1058, u'\"2014-11-24 19:20:43\"', 0, u'\"76109\"'],\n",
       "  [127, u'\"2014-10-18 12:43:23\"', 0, u'\"48895\"'],\n",
       "  [1133, u'\"2014-12-15 16:57:01\"', 1, u'\"54901\"'],\n",
       "  [637, u'\"2014-10-06 19:36:55\"', 0, u'\"42361\"'],\n",
       "  [461, u'\"2014-10-10 09:36:41\"', 0, u'\"23922\"'],\n",
       "  [59, u'\"2014-12-06 11:34:49\"', 0, u'\"36052\"'],\n",
       "  [10, u'\"2014-12-16 08:18:57\"', 0, u'\"22948\"'],\n",
       "  [1604, u'\"2014-12-23 18:25:04\"', 0, u'\"22212\"'],\n",
       "  [27, u'\"2014-10-30 16:37:58\"', 0, u'\"25651\"'],\n",
       "  [252, u'\"2014-10-08 16:00:37\"', 1, u'\"20105\"'],\n",
       "  [1342, u'\"2014-10-31 20:07:16\"', 1, u'\"17081\"'],\n",
       "  [26, u'\"2014-10-08 11:59:16\"', 0, u'\"46201\"'],\n",
       "  [1049, u'\"2014-11-16 00:54:27\"', 1, u'\"67062\"'],\n",
       "  [538, u'\"2014-10-06 21:28:27\"', 1, u'\"62726\"'],\n",
       "  [42, u'\"2014-12-21 21:09:58\"', 1, u'\"15460\"'],\n",
       "  [11, u'\"2014-12-10 18:25:12\"', 0, u'\"95550\"'],\n",
       "  [678, u'\"2014-12-14 22:16:16\"', 0, u'\"58640\"'],\n",
       "  [197, u'\"2014-11-10 20:26:37\"', 1, u'\"14720\"'],\n",
       "  [132, u'\"2014-12-23 12:04:02\"', 0, u'\"42211\"'],\n",
       "  [139, u'\"2014-11-15 02:34:54\"', 0, u'\"89155\"'],\n",
       "  [297, u'\"2014-11-13 05:38:14\"', 0, u'\"23187\"'],\n",
       "  [242, u'\"2014-10-17 15:18:15\"', 0, u'\"75469\"'],\n",
       "  [42, u'\"2014-11-01 13:23:21\"', 0, u'\"15866\"'],\n",
       "  [1088, u'\"2014-10-04 18:05:18\"', 0, u'\"30596\"'],\n",
       "  [1202, u'\"2014-10-15 20:37:56\"', 1, u'\"60638\"'],\n",
       "  [1471, u'\"2014-10-08 14:59:54\"', 1, u'\"31144\"'],\n",
       "  [291, u'\"2014-11-01 17:55:13\"', 0, u'\"26527\"'],\n",
       "  [1094, u'\"2014-11-30 16:15:39\"', 1, u'\"02902\"'],\n",
       "  [507, u'\"2014-11-17 08:26:09\"', 1, u'\"84113\"'],\n",
       "  [15, u'\"2014-12-23 13:18:47\"', 1, u'\"21640\"'],\n",
       "  [135, u'\"2014-11-30 23:10:09\"', 0, u'\"27278\"'],\n",
       "  [1022, u'\"2014-12-08 22:30:57\"', 0, u'\"73080\"'],\n",
       "  [1532, u'\"2014-10-09 19:46:01\"', 1, u'\"14054\"'],\n",
       "  [325, u'\"2014-10-17 08:20:19\"', 0, u'\"61732\"'],\n",
       "  [189, u'\"2014-10-10 04:16:10\"', 0, u'\"96064\"'],\n",
       "  [33, u'\"2014-12-15 03:10:46\"', 0, u'\"43727\"'],\n",
       "  [1041, u'\"2014-12-15 00:58:55\"', 0, u'\"20733\"'],\n",
       "  [687, u'\"2014-10-24 06:38:22\"', 1, u'\"99217\"'],\n",
       "  [187, u'\"2014-11-23 05:36:07\"', 1, u'\"60435\"'],\n",
       "  [538, u'\"2014-10-01 22:48:27\"', 1, u'\"71221\"'],\n",
       "  [305, u'\"2014-10-15 07:53:42\"', 1, u'\"92398\"'],\n",
       "  [0, u'\"2014-10-26 11:08:16\"', 0, u'\"23093\"'],\n",
       "  [1590, u'\"2014-11-25 11:36:53\"', 1, u'\"07184\"'],\n",
       "  [546, u'\"2014-10-17 08:56:56\"', 0, u'\"63441\"'],\n",
       "  [980, u'\"2014-12-29 09:29:44\"', 0, u'\"85939\"'],\n",
       "  [920, u'\"2014-11-27 18:26:59\"', 1, u'\"02478\"'],\n",
       "  [345, u'\"2014-10-15 13:20:48\"', 0, u'\"67109\"'],\n",
       "  [449, u'\"2014-10-31 17:37:49\"', 1, u'\"35231\"'],\n",
       "  [762, u'\"2014-12-06 15:33:17\"', 1, u'\"64503\"'],\n",
       "  [35, u'\"2014-10-31 15:14:03\"', 1, u'\"30439\"'],\n",
       "  [129, u'\"2014-10-10 06:32:07\"', 0, u'\"98933\"'],\n",
       "  [311, u'\"2014-10-08 17:30:50\"', 1, u'\"47427\"'],\n",
       "  [438, u'\"2014-11-17 01:28:39\"', 0, u'\"95113\"'],\n",
       "  [45, u'\"2014-10-21 18:11:21\"', 0, u'\"83849\"'],\n",
       "  [20, u'\"2014-10-24 12:20:17\"', 1, u'\"58259\"'],\n",
       "  [681, u'\"2014-12-23 22:13:00\"', 1, u'\"45876\"'],\n",
       "  [1186, u'\"2014-11-16 03:35:17\"', 0, u'\"75394\"'],\n",
       "  [339, u'\"2014-11-02 00:41:59\"', 1, u'\"49073\"'],\n",
       "  [420, u'\"2014-12-21 17:30:56\"', 1, u'\"01244\"'],\n",
       "  [416, u'\"2014-10-30 12:55:48\"', 1, u'\"55020\"'],\n",
       "  [555, u'\"2014-12-03 17:18:03\"', 0, u'\"94505\"'],\n",
       "  [637, u'\"2014-10-19 08:52:45\"', 0, u'\"13220\"'],\n",
       "  [1525, u'\"2014-12-01 04:29:12\"', 1, u'\"51636\"'],\n",
       "  [57, u'\"2014-10-09 13:00:37\"', 1, u'\"77726\"'],\n",
       "  [879, u'\"2014-12-29 19:32:28\"', 0, u'\"47522\"'],\n",
       "  [152, u'\"2014-10-20 05:21:36\"', 1, u'\"15455\"'],\n",
       "  [1295, u'\"2014-12-28 03:04:40\"', 1, u'\"49868\"'],\n",
       "  [1223, u'\"2014-11-01 21:59:31\"', 0, u'\"58062\"'],\n",
       "  [756, u'\"2014-10-16 13:50:32\"', 1, u'\"75401\"'],\n",
       "  [1194, u'\"2014-10-17 01:48:50\"', 0, u'\"68421\"']])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tracks.csv file\n",
    "tracks_file = sc.textFile(\"./raw/tracks.csv\")\n",
    "\n",
    "def make_tracks_kv(str):\n",
    "    line = str.split(\",\")\n",
    "    # Create PairRDD with line is tuple data: key is line[1]: customer ID\n",
    "    return [line[1], [[int(line[2]), line[3], int(line[4]), line[5]]]]\n",
    "\n",
    "# Make a key,value RDD out of the input data\n",
    "tracks_rdd = tracks_file.map(lambda line: make_tracks_kv(line)) \\\n",
    "                        .reduceByKey(lambda a,b: a+b)\n",
    "tracks_rdd.first() \n",
    "\n",
    "#tracks_rdd = tracks_file.map(lambda line: line.split(\",\")) \\\n",
    "#                        .map(lambda p: [p[1], [[int(p[2]), p[3], int(p[4]), p[5]]]]) \\\n",
    "#                        .reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual track events are now stored in a PairRDD, with the customer ID as the key. A summary profile can now be computed for each user, which will include:\n",
    "\n",
    "    Average number of tracks during each period of the day (time ranges are arbitrarily defined in the code)\n",
    "    Total unique tracks, i.e., the set of unique track IDs\n",
    "    Total mobile tracks, i.e., tracks played when the mobile flag was set\n",
    "\n",
    "By passing a function to mapValues, a high-level profile can be computed from the components. The summary data is now readily available to compute basic statistics that can be used for display, using the colStats function from pyspark.mllib.stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats_by_user(tracks):\n",
    "    mcount = morning = afternoon = evening = night = 0\n",
    "    tracklist = []\n",
    "    for t in tracks:\n",
    "        trackid, dtime, mobile, zip = t\n",
    "        if trackid not in tracklist:\n",
    "            tracklist.append(trackid)\n",
    "        d,t = dtime.split(\" \")\n",
    "        hour_of_day = int(t.split(\":\")[0])\n",
    "        # Compute total mobile track\n",
    "        mcount += mobile\n",
    "        # Compute number of tracks during each period of the day\n",
    "        if (hour_of_day < 5):\n",
    "            night += 1\n",
    "        elif (hour_of_day < 12):\n",
    "            morning += 1\n",
    "        elif (hour_of_day < 17):\n",
    "            afternoon += 1\n",
    "        elif (hour_of_day < 22):\n",
    "            evening += 1\n",
    "        else:\n",
    "            night += 1\n",
    "        # len(tracklist): total unique track, mcount: total mobile track\n",
    "        return [len(tracklist), morning, afternoon, evening, night, mcount]\n",
    "\n",
    "# Compute profile for each user\n",
    "# mapValues(): Pass each value in the key-value pair RDD through a map function\n",
    "# without changing the keys; this also retains the original RDD’s partitioning.\n",
    "customer_data = tracks_rdd.mapValues(lambda a: compute_stats_by_user(a))\n",
    "\n",
    "# Compute aggregate stats for entire track history\n",
    "# Compute basic statistics that can be used for display\n",
    "# like the mean and variance for each of the fields in customer_data RDD\n",
    "aggregate_data = Statistics.colStats(customer_data.map(lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line provides meaningful statistics like the mean and variance for each of the fields in the per-user RDDs that were created in custdata.\n",
    "\n",
    "Calling collect() on this RDD will persist the results back to a file. The results could be stored in a database such as MapR-DB, HBase or an RDBMS (using a Python package like happybase or dbset). For the sake of simplicity for this example, using CSV is the optimal choice. There are two files to output:\n",
    "\n",
    "    live_table.csv containing the latest calculations\n",
    "    agg_table.csv containing the aggregated data about all customers computed with Statistics.colStats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write latest calculations to a file\n",
    "with open(\"./raw/live_table.csv\", \"wb\") as live_file:\n",
    "    fwriter = csv.writer(live_file, delimiter = ' ',\n",
    "                        quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "    for key, value in customer_data.collect():\n",
    "        unique, morning, afternoon, evening, night, mobile = value\n",
    "        total_hour = morning + afternoon + evening + night\n",
    "        fwriter.writerow([unique, morning, afternoon, evening, night, mobile])\n",
    "\n",
    "# write aggregated data about all customers computed with Statistics.colStats\n",
    "with open(\"./raw/agg_table.csv\", \"wb\") as agg_file:\n",
    "    fwriter = csv.writer(agg_file, delimiter = ' ',\n",
    "                        quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "    fwriter.writerow([aggregate_data.mean()[0], aggregate_data.mean()[1],\n",
    "                        aggregate_data.mean()[2], aggregate_data.mean()[3],\n",
    "                        aggregate_data.mean()[4], aggregate_data.mean()[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
